{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import random\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import bs4\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of elements found: 1\n",
      "number of elements found: 2\n",
      "number of elements found: 3\n",
      "number of elements found: 4\n",
      "number of elements found: 5\n",
      "number of elements found: 6\n",
      "number of elements found: 7\n",
      "number of elements found: 8\n",
      "number of elements found: 9\n",
      "number of elements found: 10\n",
      "number of elements found: 11\n",
      "number of elements found: 12\n",
      "number of elements found: 13\n",
      "number of elements found: 14\n",
      "number of elements found: 15\n",
      "number of elements found: 16\n",
      "number of elements found: 17\n",
      "number of elements found: 18\n",
      "number of elements found: 19\n",
      "number of elements found: 20\n",
      "number of elements found: 21\n",
      "number of elements found: 22\n",
      "number of elements found: 23\n",
      "number of elements found: 24\n",
      "number of elements found: 25\n",
      "number of elements found: 26\n",
      "number of elements found: 27\n",
      "number of elements found: 28\n",
      "number of elements found: 29\n",
      "number of elements found: 30\n",
      "number of elements found: 31\n",
      "number of elements found: 32\n",
      "number of elements found: 33\n",
      "number of elements found: 34\n",
      "number of elements found: 35\n",
      "number of elements found: 36\n",
      "number of elements found: 37\n",
      "number of elements found: 38\n",
      "number of elements found: 39\n",
      "number of elements found: 40\n",
      "number of elements found: 41\n",
      "number of elements found: 42\n",
      "number of elements found: 43\n",
      "number of elements found: 44\n",
      "number of elements found: 45\n",
      "number of elements found: 46\n",
      "number of elements found: 47\n",
      "number of elements found: 48\n",
      "number of elements found: 49\n",
      "number of elements found: 50\n",
      "number of elements found: 51\n",
      "number of elements found: 52\n",
      "number of elements found: 53\n",
      "number of elements found: 54\n",
      "number of elements found: 55\n",
      "number of elements found: 56\n",
      "number of elements found: 57\n",
      "number of elements found: 58\n",
      "number of elements found: 59\n",
      "number of elements found: 60\n",
      "number of elements found: 61\n",
      "number of elements found: 62\n",
      "number of elements found: 63\n",
      "number of elements found: 64\n",
      "number of elements found: 65\n",
      "number of elements found: 66\n",
      "number of elements found: 67\n",
      "number of elements found: 68\n",
      "number of elements found: 69\n",
      "number of elements found: 70\n",
      "number of elements found: 71\n",
      "number of elements found: 72\n",
      "number of elements found: 73\n",
      "number of elements found: 74\n",
      "number of elements found: 75\n",
      "number of elements found: 76\n",
      "number of elements found: 77\n",
      "number of elements found: 78\n",
      "number of elements found: 79\n",
      "number of elements found: 80\n",
      "number of elements found: 81\n",
      "number of elements found: 82\n",
      "number of elements found: 83\n",
      "number of elements found: 84\n",
      "number of elements found: 85\n",
      "number of elements found: 86\n",
      "number of elements found: 87\n",
      "number of elements found: 88\n",
      "number of elements found: 89\n",
      "number of elements found: 90\n",
      "number of elements found: 91\n",
      "number of elements found: 92\n",
      "number of elements found: 93\n",
      "number of elements found: 94\n",
      "number of elements found: 95\n",
      "number of elements found: 96\n",
      "number of elements found: 97\n",
      "number of elements found: 98\n",
      "number of elements found: 99\n",
      "number of elements found: 100\n",
      "number of elements found: 101\n",
      "number of elements found: 102\n",
      "number of elements found: 103\n",
      "number of elements found: 104\n",
      "number of elements found: 105\n",
      "number of elements found: 106\n",
      "number of elements found: 107\n",
      "number of elements found: 108\n",
      "number of elements found: 109\n",
      "number of elements found: 110\n",
      "number of elements found: 111\n",
      "number of elements found: 112\n",
      "number of elements found: 113\n",
      "number of elements found: 114\n",
      "number of elements found: 115\n",
      "number of elements found: 116\n",
      "number of elements found: 117\n",
      "number of elements found: 118\n",
      "number of elements found: 119\n",
      "number of elements found: 120\n",
      "number of elements found: 121\n",
      "number of elements found: 122\n",
      "number of elements found: 123\n",
      "number of elements found: 124\n",
      "number of elements found: 125\n",
      "number of elements found: 126\n",
      "number of elements found: 127\n",
      "number of elements found: 128\n",
      "number of elements found: 129\n",
      "number of elements found: 130\n",
      "number of elements found: 131\n",
      "number of elements found: 132\n",
      "number of elements found: 133\n",
      "number of elements found: 134\n",
      "number of elements found: 135\n",
      "number of elements found: 136\n",
      "number of elements found: 137\n",
      "number of elements found: 138\n",
      "number of elements found: 139\n",
      "number of elements found: 140\n",
      "number of elements found: 141\n",
      "number of elements found: 142\n",
      "number of elements found: 143\n",
      "number of elements found: 144\n",
      "number of elements found: 145\n",
      "number of elements found: 146\n",
      "number of elements found: 147\n",
      "number of elements found: 148\n",
      "number of elements found: 149\n",
      "number of elements found: 150\n",
      "number of elements found: 151\n",
      "number of elements found: 152\n",
      "number of elements found: 153\n",
      "number of elements found: 154\n",
      "number of elements found: 155\n",
      "number of elements found: 156\n",
      "number of elements found: 157\n",
      "number of elements found: 158\n",
      "number of elements found: 159\n",
      "number of elements found: 160\n",
      "number of elements found: 161\n",
      "number of elements found: 162\n",
      "number of elements found: 163\n",
      "number of elements found: 164\n",
      "number of elements found: 165\n",
      "number of elements found: 166\n",
      "number of elements found: 167\n",
      "number of elements found: 168\n",
      "number of elements found: 169\n",
      "number of elements found: 170\n",
      "number of elements found: 171\n",
      "number of elements found: 172\n",
      "number of elements found: 173\n",
      "number of elements found: 174\n",
      "number of elements found: 175\n",
      "number of elements found: 176\n",
      "number of elements found: 177\n",
      "number of elements found: 178\n",
      "number of elements found: 179\n",
      "number of elements found: 180\n",
      "number of elements found: 181\n",
      "number of elements found: 182\n",
      "number of elements found: 183\n",
      "number of elements found: 184\n",
      "number of elements found: 185\n",
      "number of elements found: 186\n",
      "number of elements found: 187\n",
      "number of elements found: 188\n",
      "number of elements found: 189\n",
      "number of elements found: 190\n",
      "number of elements found: 191\n",
      "number of elements found: 192\n",
      "number of elements found: 193\n",
      "number of elements found: 194\n",
      "number of elements found: 195\n",
      "number of elements found: 196\n",
      "number of elements found: 197\n",
      "number of elements found: 198\n",
      "number of elements found: 199\n",
      "number of elements found: 200\n",
      "Elements loaded 200, breaking the loop.\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.glassdoor.co.in/Job/india-data-analyst-jobs-SRCH_IL.0,5_IN115_KO6,18.htm?suggestCount=0&suggestChosen=false&clickSource=searchBtn&typedKeyword=Data%2520&typedLocation=India&context=Jobs&dropdown=0'\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(url)\n",
    "\n",
    "# create empty hrml list\n",
    "\n",
    "html_list = []\n",
    "\n",
    "# initialize the number of elements loaded to be zero\n",
    "\n",
    "num_elements_loaded = 0\n",
    "\n",
    "# intialising the loop\n",
    "\n",
    "while True:\n",
    "    timeout = random.randint(2, 5) # seting a random value for the timeout for every iteration\n",
    "\n",
    "    # The num_elements variable is used to count the number of elements that have been loaded.\n",
    "\n",
    "    num_elements = len(driver.find_elements(by=By.CSS_SELECTOR, value=\".css-1vg6q84.e1tk4kwz4\"))\n",
    "    num_elements_loaded += num_elements # add the len of num_elemment to number of elements loaded in every iteraion\n",
    "    print(f'number of elements found: {num_elements_loaded}')\n",
    "\n",
    "    # If 500 elements are loaded, break the loop\n",
    "\n",
    "    if num_elements_loaded >= 200:\n",
    "        print(f'Elements loaded {num_elements_loaded}, breaking the loop.')\n",
    "        break\n",
    "\n",
    "    # # Wait for a random amount of time before reloading more elements.\n",
    "\n",
    "    time.sleep(timeout) \n",
    "    \n",
    "    # Scroll down to load more elements\n",
    "\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "    # Wait for more elements to load\n",
    "\n",
    "    try:\n",
    "        WebDriverWait(driver, 1000).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \".css-1vg6q84.e1tk4kwz4\")))\n",
    "    except TimeoutException:\n",
    "        # If timeout occurs, increase the timeout and continue scrolling\n",
    "\n",
    "        continue\n",
    "\n",
    "    # Store the HTML of the current scroll in a list\n",
    "\n",
    "    html_list.append(driver.page_source)\n",
    "\n",
    "# quit the driver\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, html in enumerate(html_list):\n",
    "    # Parse the HTML string using BeautifulSoup\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    # Write the prettified HTML to a file with a unique filename\n",
    "    with open(f'glassdoor_output{i}.html', 'w', encoding='utf-8') as file:\n",
    "        file.write(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the extraction functions\n",
    "\n",
    "def text_extractor(node):\n",
    "    return node.text\n",
    "def string_extractor(node):\n",
    "    if type(node.contents[0]) is bs4.element.NavigableString:\n",
    "        return node.contents[0]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create classes dictionary\n",
    "\n",
    "classes = {\n",
    "    'rating': {\n",
    "        'element': 'span',\n",
    "        'cls': 'css-2lqh28 e1cjmv6j1',\n",
    "        'extr': string_extractor\n",
    "    },\n",
    "    'company_name': {\n",
    "        'element': 'a',\n",
    "        'cls': 'css-l2wjgv e1n63ojh0 jobLink',\n",
    "        'extr': text_extractor\n",
    "    },\n",
    "    'job_position': {\n",
    "        'element': 'a',\n",
    "        'cls': 'jobLink css-1rd3saf eigr9kq2',\n",
    "        'extr': text_extractor\n",
    "    },\n",
    "    'location': {\n",
    "        'element': 'span',\n",
    "        'cls': 'css-3g3psg pr-xxsm css-iii9i8 e1rrn5ka0',\n",
    "        'extr': text_extractor\n",
    "    },\n",
    "    'salary_estimate': {\n",
    "        'element': 'span',\n",
    "        'cls': 'css-1xe2xww e1wijj242',\n",
    "        'extr': text_extractor\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store the extracted data\n",
    "data = []\n",
    "\n",
    "# Loop through the html files and extract data from each file\n",
    "for i in range(199):\n",
    "    filename = f'glassdoor_output{i}.html'\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        html = file.read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    result = {}\n",
    "\n",
    "    # Loop through the classes dictionary and extract data for each class\n",
    "    for key in classes:\n",
    "        collection = soup.find_all(classes[key]['element'], class_=classes[key]['cls'])\n",
    "        if collection:\n",
    "            result[key] = [classes[key]['extr'](c) for c in collection]\n",
    "        else:\n",
    "            result[key] = [None] * len(data) # Fill in missing values with None\n",
    "\n",
    "    # Ensure that the length of the extracted data is consistent for all classes\n",
    "    max_len = max([len(result[key]) for key in result])\n",
    "    for key in result:\n",
    "        if len(result[key]) < max_len:\n",
    "            result[key] += [None] * (max_len - len(result[key]))\n",
    "\n",
    "    # result['file'] = filename # Add a new column to indicate which file the data is from\n",
    "    data.append(result) # Append the result dictionary to the data list\n",
    "\n",
    "# Concatenate the data from all files into a single DataFrame\n",
    "df = pd.concat([pd.DataFrame(d) for d in data], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>rating</th>\n",
       "      <th>company_name</th>\n",
       "      <th>job_position</th>\n",
       "      <th>location</th>\n",
       "      <th>salary_estimate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4.1</td>\n",
       "      <td>S&amp;P Global</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>India</td>\n",
       "      <td>₹3L - ₹6L                    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4.2</td>\n",
       "      <td>Airtel India</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>₹5L - ₹9L                    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Barclays</td>\n",
       "      <td>Analyst - RFT Data Operations ...</td>\n",
       "      <td>New Delhi</td>\n",
       "      <td>₹8L - ₹8L                    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4.2</td>\n",
       "      <td>Airtel India</td>\n",
       "      <td>Business Analyst - Messaging  ...</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>₹40T - ₹45T                  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3.2</td>\n",
       "      <td>Glamyo Health</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Connaught Place</td>\n",
       "      <td>₹7L - ₹11L                   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5965</th>\n",
       "      <td>5965</td>\n",
       "      <td>None</td>\n",
       "      <td>Airtel India</td>\n",
       "      <td>Analyst</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5966</th>\n",
       "      <td>5966</td>\n",
       "      <td>None</td>\n",
       "      <td>Indigifts Pvt. Ltd.          ...</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Jaipur</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5967</th>\n",
       "      <td>5967</td>\n",
       "      <td>None</td>\n",
       "      <td>BNY Mellon</td>\n",
       "      <td>Analyst, Data Management &amp; Qua...</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5968</th>\n",
       "      <td>5968</td>\n",
       "      <td>None</td>\n",
       "      <td>SSPL</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>New Delhi</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5969</th>\n",
       "      <td>5969</td>\n",
       "      <td>None</td>\n",
       "      <td>Traya Health</td>\n",
       "      <td>Consumer Insights &amp; Data Analy...</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5970 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                             rating  \\\n",
       "0         0                 4.1                  \n",
       "1         1                 4.2                  \n",
       "2         2                 4.0                  \n",
       "3         3                 4.2                  \n",
       "4         4                 3.2                  \n",
       "...     ...                                ...   \n",
       "5965   5965                               None   \n",
       "5966   5966                               None   \n",
       "5967   5967                               None   \n",
       "5968   5968                               None   \n",
       "5969   5969                               None   \n",
       "\n",
       "                                           company_name  \\\n",
       "0                            S&P Global                   \n",
       "1                          Airtel India                   \n",
       "2                              Barclays                   \n",
       "3                          Airtel India                   \n",
       "4                         Glamyo Health                   \n",
       "...                                                 ...   \n",
       "5965                       Airtel India                   \n",
       "5966                   Indigifts Pvt. Ltd.          ...   \n",
       "5967                         BNY Mellon                   \n",
       "5968                               SSPL                   \n",
       "5969                       Traya Health                   \n",
       "\n",
       "                                           job_position  \\\n",
       "0                           Data Analyst                  \n",
       "1                           Data Analyst                  \n",
       "2                     Analyst - RFT Data Operations ...   \n",
       "3                     Business Analyst - Messaging  ...   \n",
       "4                           Data Analyst                  \n",
       "...                                                 ...   \n",
       "5965                             Analyst                  \n",
       "5966                        Data Analyst                  \n",
       "5967                  Analyst, Data Management & Qua...   \n",
       "5968                        Data Analyst                  \n",
       "5969                  Consumer Insights & Data Analy...   \n",
       "\n",
       "                                            location  \\\n",
       "0                               India                  \n",
       "1                             Gurgaon                  \n",
       "2                           New Delhi                  \n",
       "3                             Gurgaon                  \n",
       "4                     Connaught Place                  \n",
       "...                                              ...   \n",
       "5965                          Gurgaon                  \n",
       "5966                           Jaipur                  \n",
       "5967                      Maharashtra                  \n",
       "5968                        New Delhi                  \n",
       "5969                           Mumbai                  \n",
       "\n",
       "                                        salary_estimate  \n",
       "0                      ₹3L - ₹6L                    ...  \n",
       "1                      ₹5L - ₹9L                    ...  \n",
       "2                      ₹8L - ₹8L                    ...  \n",
       "3                      ₹40T - ₹45T                  ...  \n",
       "4                      ₹7L - ₹11L                   ...  \n",
       "...                                                 ...  \n",
       "5965                                               None  \n",
       "5966                                               None  \n",
       "5967                                               None  \n",
       "5968                                               None  \n",
       "5969                                               None  \n",
       "\n",
       "[5970 rows x 6 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace \\n from the dataframe\n",
    "\n",
    "df = df.replace('\\n', '', regex=True)\n",
    "df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the DataFrame to a CSV file\n",
    "df.to_csv('glassdoor_data_analyst_jobs_backup.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jobs(keyword, num_jobs, verbose):\n",
    "    \n",
    "    '''Gathers jobs as a dataframe, scraped from Glassdoor'''\n",
    "    \n",
    "    #Initializing the webdriver\n",
    "    options = webdriver.ChromeOptions()\n",
    "    \n",
    "    #Uncomment the line below if you'd like to scrape without a new Chrome window every time.\n",
    "    #options.add_argument('headless')\n",
    "    \n",
    "    #Change the path to where chromedriver is in your home folder.\n",
    "    driver = webdriver.Chrome(executable_path=\"C:/Users/9ayus/Documents/da_salary_project/chromedriver\", options=options)\n",
    "    driver.set_window_size(1120, 1000)\n",
    "\n",
    "    url = 'https://www.glassdoor.com/Job/jobs.htm?sc.keyword=\"' + keyword + '\"&locT=C&locId=1147401&locKeyword=San%20Francisco,%20CA&jobType=all&fromAge=-1&minSalary=0&includeNoSalaryJobs=true&radius=100&cityId=-1&minRating=0.0&industryId=-1&sgocId=-1&seniorityType=all&companyId=-1&employerSizes=0&applicationType=0&remoteWorkType=0'\n",
    "    driver.get(url)\n",
    "    jobs = []\n",
    "\n",
    "    while len(jobs) < num_jobs:  #If true, should be still looking for new jobs.\n",
    "\n",
    "        #Let the page load. Change this number based on your internet speed.\n",
    "        #Or, wait until the webpage is loaded, instead of hardcoding it.\n",
    "        time.sleep(10)\n",
    "\n",
    "        #Test for the \"Sign Up\" prompt and get rid of it.\n",
    "        try:\n",
    "            driver.find_element(By.CLASS_NAME, \"selected\").click()\n",
    "        except ElementClickInterceptedException:\n",
    "            pass\n",
    "\n",
    "        time.sleep(.1)\n",
    "\n",
    "        try:\n",
    "            driver.find_element(By.CLASS_NAME, \"ModalStyle__xBtn___29PT9\").click()  #clicking to the X.\n",
    "        except NoSuchElementException:\n",
    "            pass\n",
    "\n",
    "        \n",
    "        #Going through each job in this page\n",
    "        job_buttons = driver.find_elements(By.CLASS_NAME, \"jl\")  #jl for Job Listing. These are the buttons we're going to click.\n",
    "        for job_button in job_buttons:  \n",
    "\n",
    "            print(\"Progress: {}\".format(\"\" + str(len(jobs)) + \"/\" + str(num_jobs)))\n",
    "            if len(jobs) >= num_jobs:\n",
    "                break\n",
    "\n",
    "            job_button.click()  #You might \n",
    "            time.sleep(5)\n",
    "            collected_successfully = False\n",
    "            \n",
    "            while not collected_successfully:\n",
    "                try:\n",
    "                    company_name = driver.find_element(By.XPATH, './/div[@class=\"employerName\"]').text\n",
    "                    location = driver.find_element(By.XPATH, './/div[@class=\"location\"]').text\n",
    "                    job_title = driver.find_element(By.XPATH, './/div[contains(@class, \"title\")]').text\n",
    "                    job_description = driver.find_element(By.XPATH, './/div[@class=\"jobDescriptionContent desc\"]').text\n",
    "                    collected_successfully = True\n",
    "                except:\n",
    "                    time.sleep(10)\n",
    "\n",
    "            try:\n",
    "                salary_estimate = driver.find_element(By.XPATH, './/span[@class=\"gray small salary\"]').text\n",
    "            except NoSuchElementException:\n",
    "                salary_estimate = -1 #You need to set a \"not found value. It's important.\"\n",
    "            \n",
    "            try:\n",
    "                rating = driver.find_element(By.XPATH, './/span[@class=\"rating\"]').text\n",
    "            except NoSuchElementException:\n",
    "                rating = -1 #You need to set a \"not found value. It's important.\"\n",
    "\n",
    "            #Printing for debugging\n",
    "            if verbose:\n",
    "                print(\"Job Title: {}\".format(job_title))\n",
    "                print(\"Salary Estimate: {}\".format(salary_estimate))\n",
    "                print(\"Job Description: {}\".format(job_description[:500]))\n",
    "                print(\"Rating: {}\".format(rating))\n",
    "                print(\"Company Name: {}\".format(company_name))\n",
    "                print(\"Location: {}\".format(location))\n",
    "\n",
    "            #Going to the Company tab...\n",
    "            #clicking on this:\n",
    "            #<div class=\"tab\" data-tab-type=\"overview\"><span>Company</span></div>\n",
    "            try:\n",
    "                driver.find_element(By.XPATH, './/div[@class=\"tab\" and @data-tab-type=\"overview\"]').click()\n",
    "\n",
    "                try:\n",
    "                    #<div class=\"infoEntity\">\n",
    "                    #    <label>Headquarters</label>\n",
    "                    #    <span class=\"value\">San Francisco, CA</span>\n",
    "                    #</div>\n",
    "                    headquarters = driver.find_element(By.XPATH, './/div[@class=\"infoEntity\"]//label[text()=\"Headquarters\"]//following-sibling::*').text\n",
    "                except NoSuchElementException:\n",
    "                    headquarters = -1\n",
    "\n",
    "                try:\n",
    "                    size = driver.find_element(By.XPATH, './/div[@class=\"infoEntity\"]//label[text()=\"Size\"]//following-sibling::*').text\n",
    "                except NoSuchElementException:\n",
    "                    size = -1\n",
    "\n",
    "                try:\n",
    "                    founded = driver.find_element(By.XPATH, './/div[@class=\"infoEntity\"]//label[text()=\"Founded\"]//following-sibling::*').text\n",
    "                except NoSuchElementException:\n",
    "                    founded = -1\n",
    "\n",
    "                try:\n",
    "                    type_of_ownership = driver.find_element(By.XPATH, './/div[@class=\"infoEntity\"]//label[text()=\"Type\"]//following-sibling::*').text\n",
    "                except NoSuchElementException:\n",
    "                    type_of_ownership = -1\n",
    "\n",
    "                try:\n",
    "                    industry = driver.find_element(By.XPATH, './/div[@class=\"infoEntity\"]//label[text()=\"Industry\"]//following-sibling::*').text\n",
    "                except NoSuchElementException:\n",
    "                    industry = -1\n",
    "\n",
    "                try:\n",
    "                    sector = driver.find_element(By.XPATH, './/div[@class=\"infoEntity\"]//label[text()=\"Sector\"]//following-sibling::*').text\n",
    "                except NoSuchElementException:\n",
    "                    sector = -1\n",
    "\n",
    "                try:\n",
    "                    revenue = driver.find_element(By.XPATH, './/div[@class=\"infoEntity\"]//label[text()=\"Revenue\"]//following-sibling::*').text\n",
    "                except NoSuchElementException:\n",
    "                    revenue = -1\n",
    "\n",
    "                try:\n",
    "                    competitors = driver.find_element(By.XPATH, './/div[@class=\"infoEntity\"]//label[text()=\"Competitors\"]//following-sibling::*').text\n",
    "                except NoSuchElementException:\n",
    "                    competitors = -1\n",
    "\n",
    "            except NoSuchElementException:  #Rarely, some job postings do not have the \"Company\" tab.\n",
    "                headquarters = -1\n",
    "                size = -1\n",
    "                founded = -1\n",
    "                type_of_ownership = -1\n",
    "                industry = -1\n",
    "                sector = -1\n",
    "                revenue = -1\n",
    "                competitors = -1\n",
    "\n",
    "                \n",
    "            if verbose:\n",
    "                print(\"Headquarters: {}\".format(headquarters))\n",
    "                print(\"Size: {}\".format(size))\n",
    "                print(\"Founded: {}\".format(founded))\n",
    "                print(\"Type of Ownership: {}\".format(type_of_ownership))\n",
    "                print(\"Industry: {}\".format(industry))\n",
    "                print(\"Sector: {}\".format(sector))\n",
    "                print(\"Revenue: {}\".format(revenue))\n",
    "                print(\"Competitors: {}\".format(competitors))\n",
    "                print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\")\n",
    "\n",
    "            jobs.append({\"Job Title\" : job_title,\n",
    "            \"Salary Estimate\" : salary_estimate,\n",
    "            \"Job Description\" : job_description,\n",
    "            \"Rating\" : rating,\n",
    "            \"Company Name\" : company_name,\n",
    "            \"Location\" : location,\n",
    "            \"Headquarters\" : headquarters,\n",
    "            \"Size\" : size,\n",
    "            \"Founded\" : founded,\n",
    "            \"Type of ownership\" : type_of_ownership,\n",
    "            \"Industry\" : industry,\n",
    "            \"Sector\" : sector,\n",
    "            \"Revenue\" : revenue,\n",
    "            \"Competitors\" : competitors})\n",
    "            #add job to jobs\n",
    "\n",
    "        #Clicking on the \"next page\" button\n",
    "        try:\n",
    "            driver.find_element(By.XPATH, './/li[@class=\"next\"]//a').click()\n",
    "        except NoSuchElementException:\n",
    "            print(\"Scraping terminated before reaching target number of jobs. Needed {}, got {}.\".format(num_jobs, len(jobs)))\n",
    "            break\n",
    "\n",
    "    return pd.DataFrame(jobs)  #This line converts the dictionary object into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\9ayus\\AppData\\Local\\Temp\\ipykernel_17732\\3882706317.py:12: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(executable_path=\"C:/Users/9ayus/Documents/da_salary_project/chromedriver\", options=options)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping terminated before reaching target number of jobs. Needed 5, got 0.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This line will open a new chrome window and start the scraping.\n",
    "df = get_jobs(\"data scientist\", 5, False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Send a request to the webpage\n",
    "url = 'https://www.glassdoor.co.in/Job/india-data-analyst-jobs-SRCH_IL.0,5_IN115_KO6,18.htm?suggestCount=0&suggestChosen=false&clickSource=searchBtn&typedKeyword=Data%2520&typedLocation=India&context=Jobs&dropdown=0'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find all the clickable elements on the page\n",
    "clickable_elements = soup.find_all(['a', 'button'], href=True, onclick=True)\n",
    "for element in clickable_elements:\n",
    "    print(element)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import ElementClickInterceptedException, NoSuchElementException\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "def get_clickable_elements(url):\n",
    "    '''Returns all clickable elements on a webpage using BeautifulSoup'''\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    clickable_elements = soup.find_all(['a', 'button'], href=True, onclick=True)\n",
    "    return clickable_elements\n",
    "\n",
    "def get_jobs(keyword, num_jobs, verbose):\n",
    "    \n",
    "    '''Gathers jobs as a dataframe, scraped from Glassdoor'''\n",
    "    \n",
    "    # Get all clickable elements on the webpage\n",
    "    url = 'https://www.glassdoor.com/Job/jobs.htm?'\n",
    "    url_params = {\n",
    "        'sc.keyword': keyword,\n",
    "        'locT': 'C',\n",
    "        'locId': '1147401',\n",
    "        'locKeyword': 'San Francisco, CA',\n",
    "        'jobType': 'all',\n",
    "        'fromAge': '-1',\n",
    "        'minSalary': '0',\n",
    "        'includeNoSalaryJobs': 'true',\n",
    "        'radius': '100',\n",
    "        'cityId': '-1',\n",
    "        'minRating': '0.0',\n",
    "        'industryId': '-1',\n",
    "        'sgocId': '-1',\n",
    "        'seniorityType': 'all',\n",
    "        'companyId': '-1',\n",
    "        'employerSizes': '0',\n",
    "        'applicationType': '0',\n",
    "        'remoteWorkType': '0'\n",
    "    }\n",
    "    url += '&'.join([f\"{k}={v}\" for k, v in url_params.items()])\n",
    "    clickable_elements = get_clickable_elements(url)\n",
    "\n",
    "    # Initializing the webdriver\n",
    "    options = webdriver.ChromeOptions()\n",
    "    \n",
    "    # Uncomment the line below if you'd like to scrape without a new Chrome window every time.\n",
    "    # options.add_argument('headless')\n",
    "    \n",
    "    # Change the path to where chromedriver is in your home folder.\n",
    "    driver = webdriver.Chrome(executable_path=\"C:/Users/9ayus/Documents/da_salary_project/chromedriver\", options=options)\n",
    "    driver.set_window_size(1120, 1000)\n",
    "\n",
    "    driver.get(url)\n",
    "    jobs = []\n",
    "    job_urls = []\n",
    "\n",
    "    while len(job_urls) < num_jobs:  # If true, should still be looking for new jobs.\n",
    "\n",
    "        # Let the page load. Change this number based on your internet speed.\n",
    "        # Or, wait until the webpage is loaded, instead of hardcoding it.\n",
    "        time.sleep(10)\n",
    "\n",
    "        # Test for the \"Sign Up\" prompt and get rid of it.\n",
    "        try:\n",
    "            driver.find_element(By.CLASS_NAME, \"selected\").click()\n",
    "        except ElementClickInterceptedException:\n",
    "            pass\n",
    "\n",
    "        time.sleep(.1)\n",
    "\n",
    "        try:\n",
    "            driver.find_element(By.CLASS_NAME, \"ModalStyle__xBtn___29PT9\").click()  # clicking to the X.\n",
    "        except NoSuchElementException:\n",
    "            pass\n",
    "\n",
    "        # Going through each job in this page\n",
    "        job_buttons = driver.find_elements(By.CLASS_NAME, \"jl\")  # jl for Job Listing. These are the buttons we're going to click.\n",
    "        for job_button in job_buttons:\n",
    "\n",
    "            if len(job_urls) >= num_jobs:\n",
    "                break\n",
    "\n",
    "            job_button.click()  # You might\n",
    "            time.sleep(5)\n",
    "            collected_successfully = False\n",
    "\n",
    "            while not collected_successfully:\n",
    "                try:\n",
    "                    company_name = driver.find_element(By.XPATH, './/div[@class=\"employerName\"]').text\n",
    "                    location = driver.find_element(By.XPATH, './/div[@class=\"location\"]').text\n",
    "                    job_title = driver.find_element(By.XPATH, './/div[contains(@class, \"title\")]').text\n",
    "                    job_description = driver.find_element(By.XPATH, './/div[@class=\"jobDescriptionContent desc\"]').text\n",
    "                    collected_successfully = True\n",
    "                except:\n",
    "                    time.sleep(10)\n",
    "\n",
    "            try:\n",
    "                salary_estimate = driver.find_element(By.XPATH, './/span[@class=\"gray small salary\"]').text\n",
    "            except NoSuchElementException:\n",
    "                salary_estimate = -1 #You need to set a \"not found value. It's important.\"\n",
    "            \n",
    "            try:\n",
    "                rating = driver.find_element(By.XPATH, './/span[@class=\"rating\"]').text\n",
    "            except NoSuchElementException:\n",
    "                rating = -1 #You need to set a \"not found value. It's important.\"\n",
    "\n",
    "            #Printing for debugging\n",
    "            if verbose:\n",
    "                print(\"Job Title: {}\".format(job_title))\n",
    "                print(\"Salary Estimate: {}\".format(salary_estimate))\n",
    "                print(\"Job Description: {}\".format(job_description[:500]))\n",
    "                print(\"Rating: {}\".format(rating))\n",
    "                print(\"Company Name: {}\".format(company_name))\n",
    "                print(\"Location: {}\".format(location))\n",
    "\n",
    "            #Going to the Company tab...\n",
    "            #clicking on this:\n",
    "            #<div class=\"tab\" data-tab-type=\"overview\"><span>Company</span></div>\n",
    "            try:\n",
    "                driver.find_element(By.XPATH, './/div[@class=\"tab\" and @data-tab-type=\"overview\"]').click()\n",
    "\n",
    "                try:\n",
    "                    #<div class=\"infoEntity\">\n",
    "                    #    <label>Headquarters</label>\n",
    "                    #    <span class=\"value\">San Francisco, CA</span>\n",
    "                    #</div>\n",
    "                    headquarters = driver.find_element(By.XPATH, './/div[@class=\"infoEntity\"]//label[text()=\"Headquarters\"]//following-sibling::*').text\n",
    "                except NoSuchElementException:\n",
    "                    headquarters = -1\n",
    "\n",
    "                try:\n",
    "                    size = driver.find_element(By.XPATH, './/div[@class=\"infoEntity\"]//label[text()=\"Size\"]//following-sibling::*').text\n",
    "                except NoSuchElementException:\n",
    "                    size = -1\n",
    "\n",
    "                try:\n",
    "                    founded = driver.find_element(By.XPATH, './/div[@class=\"infoEntity\"]//label[text()=\"Founded\"]//following-sibling::*').text\n",
    "                except NoSuchElementException:\n",
    "                    founded = -1\n",
    "\n",
    "                try:\n",
    "                    type_of_ownership = driver.find_element(By.XPATH, './/div[@class=\"infoEntity\"]//label[text()=\"Type\"]//following-sibling::*').text\n",
    "                except NoSuchElementException:\n",
    "                    type_of_ownership = -1\n",
    "\n",
    "                try:\n",
    "                    industry = driver.find_element(By.XPATH, './/div[@class=\"infoEntity\"]//label[text()=\"Industry\"]//following-sibling::*').text\n",
    "                except NoSuchElementException:\n",
    "                    industry = -1\n",
    "\n",
    "                try:\n",
    "                    sector = driver.find_element(By.XPATH, './/div[@class=\"infoEntity\"]//label[text()=\"Sector\"]//following-sibling::*').text\n",
    "                except NoSuchElementException:\n",
    "                    sector = -1\n",
    "\n",
    "                try:\n",
    "                    revenue = driver.find_element(By.XPATH, './/div[@class=\"infoEntity\"]//label[text()=\"Revenue\"]//following-sibling::*').text\n",
    "                except NoSuchElementException:\n",
    "                    revenue = -1\n",
    "\n",
    "                try:\n",
    "                    competitors = driver.find_element(By.XPATH, './/div[@class=\"infoEntity\"]//label[text()=\"Competitors\"]//following-sibling::*').text\n",
    "                except NoSuchElementException:\n",
    "                    competitors = -1\n",
    "\n",
    "            except NoSuchElementException:  #Rarely, some job postings do not have the \"Company\" tab.\n",
    "                headquarters = -1\n",
    "                size = -1\n",
    "                founded = -1\n",
    "                type_of_ownership = -1\n",
    "                industry = -1\n",
    "                sector = -1\n",
    "                revenue = -1\n",
    "                competitors = -1\n",
    "\n",
    "                \n",
    "            if verbose:\n",
    "                print(\"Headquarters: {}\".format(headquarters))\n",
    "                print(\"Size: {}\".format(size))\n",
    "                print(\"Founded: {}\".format(founded))\n",
    "                print(\"Type of Ownership: {}\".format(type_of_ownership))\n",
    "                print(\"Industry: {}\".format(industry))\n",
    "                print(\"Sector: {}\".format(sector))\n",
    "                print(\"Revenue: {}\".format(revenue))\n",
    "                print(\"Competitors: {}\".format(competitors))\n",
    "                print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\")\n",
    "\n",
    "            jobs.append({\"Job Title\" : job_title,\n",
    "            \"Salary Estimate\" : salary_estimate,\n",
    "            \"Job Description\" : job_description,\n",
    "            \"Rating\" : rating,\n",
    "            \"Company Name\" : company_name,\n",
    "            \"Location\" : location,\n",
    "            \"Headquarters\" : headquarters,\n",
    "            \"Size\" : size,\n",
    "            \"Founded\" : founded,\n",
    "            \"Type of ownership\" : type_of_ownership,\n",
    "            \"Industry\" : industry,\n",
    "            \"Sector\" : sector,\n",
    "            \"Revenue\" : revenue,\n",
    "            \"Competitors\" : competitors})\n",
    "            #add job to jobs\n",
    "\n",
    "        #Clicking on the \"next page\" button\n",
    "        try:\n",
    "            driver.find_element(By.XPATH, './/li[@class=\"next\"]//a').click()\n",
    "        except NoSuchElementException:\n",
    "            print(\"Scraping terminated before reaching target number of jobs. Needed {}, got {}.\".format(num_jobs, len(jobs)))\n",
    "            break\n",
    "\n",
    "    return pd.DataFrame(jobs)  #This line converts the dictionary object into a pandas DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\9ayus\\AppData\\Local\\Temp\\ipykernel_17732\\558897803.py:52: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(executable_path=\"C:/Users/9ayus/Documents/da_salary_project/chromedriver\", options=options)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping terminated before reaching target number of jobs. Needed 5, got 0.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This line will open a new chrome window and start the scraping.\n",
    "df = get_jobs(\"data scientist\", 5, False)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
